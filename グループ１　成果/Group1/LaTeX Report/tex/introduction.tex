\section{Introduction}
\label{sec:introduction}

Since the introduction of the transformer architecture in 2017\cite{vaswani2023attentionneed} and the release of OpenAI’s GPT models\cite{radford2018improving}\cite{radford2019language}\cite{brown2020languagemodelsfewshotlearners}, the machine learning community has primarily focused on transformer-based models. The discovery of scaling laws\cite{kaplan2020scalinglawsneurallanguage} further shifted research priorities from optimizing model efficiency to increasing model and dataset sizes. As models were trained on vast datasets sourced from the internet, scaling laws initially continued to hold, but their limitations have since been questioned. While it remains uncertain whether these laws have truly reached a ceiling, it is evident that most publicly available raw text data has already been utilized. Consequently, research efforts begun shifting toward improving model inference quality. As of February 2025, DeepSeek R1, one of the leading models known for its strong reasoning capabilities, had been further refined through reinforcement learning\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. Post-training techniques for foundational models have become vital for achieving high reasoning performance.

Moreover, recent research demonstrates that LLMs can achieve enhanced performance through techniques like prompt engineering, without updating model weights. For example, Chain of Thought (CoT) reasoning\cite{kojima2023largelanguagemodelszeroshot} improves an LLM’s ability to solve complex problems by simply instructing it to “think step by step.” Similarly, reflection prompting\cite{madaan2023selfrefineiterativerefinementselffeedback}\cite{shinn2023reflexionlanguageagentsverbal} models to think about their own thinking before finalizing an answer—has proven effective in refining their reasoning capabilities. Moreover, employing a multi-agent debate strategy, where several LLM agents engage in structured discussions, has been shown to further boost performance compared to a single-agent approach \cite{du2023improvingfactualityreasoninglanguage}.

In this study, we analyzed the performance of different multi-agent debate system hierarchies. Specifically, we compared the effectiveness of a zero-shot agent against various multi-agent configurations, assessing their impact on accuracy, computational cost, and time efficiency. Our findings provide insights into the trade-offs associated with different debate structures and highlight the advantages of multi-agent collaboration in enhancing reasoning capabilities. Furthermore, we discuss adaptive hierarchical structures that dynamically evolve based on performance metrics for more accurate multi-agent reasoning frameworks.
