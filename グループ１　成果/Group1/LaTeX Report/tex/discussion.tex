\section{Discussion}
\label{sec:discussion}

Our study suggests that incorporating hierarchical depth into the communication topology of LLM agents enhances overall accuracy by enabling multi-level collaboration and introducing redundancy to mitigate errors. While our results do not explicitly demonstrate that a deeper hierarchical communication topology (ABCCBCCC) outperforms a shallower one (ABB) in accuracy, the analysis of conversation history suggests that increased redundancy improves self-correction. This effect may become more pronounced when testing these systems on more complex problem sets. However, redundancy and computational efficiency are inherently trade-offs, and implementing redundancy within a multi-agent debate framework is computationally demanding.

Looking ahead, we propose exploring adaptive optimization methods—such as evolutionary algorithms (e.g., NEAT\cite{stanley:ec02}) or reinforcement learning—to optimize the communication topology to specific problem characteristics, although defining what constitutes a “more optimized” topology remains an open question given the high-dimensional nature of intelligence. In addition, dynamically modifying intrinsic agent properties, such as character or temperature, would promote divergent, out-of-the-box reasoning by diversifying response generation\cite{liang2024encouragingdivergentthinkinglarge}; however, this approach requires regenerating outputs for evaluation within an evolutionary framework, further increasing computational demands.